# Azure-DataFactory-SCD-Type1
This project demonstrates how to implement Slowly Changing Dimension (SCD) Type 1 using Azure Data Factory (ADF) Data Flows.

ğŸš€ Project Overview
This project demonstrates how to implement Slowly Changing Dimension (SCD) Type 1 using Azure Data Factory (ADF) Data Flows. The project involves:<br>
Setting up an Azure Blob Storage container to store employee datasets in CSV format.<br>
Creating an Azure SQL Database to store employee records.<br>
Building an ADF pipeline to initially load employee data into SQL.<br>
Implementing SCD Type 1 using ADF Data Flows, which:<br>
Overwrites the salary of employees with updated salaries.<br>
Inserts new employees into the table.


ğŸ“Œ 1. Setting Up Azure Resources<br>
1ï¸âƒ£ Creating Azure Blob Storage<br>
Created a Storage Account in Azure.<br>
Created a Blob Container named employee-data.<br>
Uploaded the following files:<br>
Employees_Initial.csv (Initial employee data)<br>
Employees_Updated.csv (Updated employee data)


2ï¸âƒ£ Creating Azure SQL Database<br>
Created an Azure SQL Database named EmployeeDB.<br>
Created a table to store employee records using the following query:<br>
CREATE TABLE Employees (<br>
    EmployeeID INT PRIMARY KEY,<br>
    Name VARCHAR(100),<br>
    Salary DECIMAL(10,2),<br>
    Location VARCHAR(100)<br>
);


ğŸ“Œ 2. Setting Up Azure Data Factory (ADF)
1ï¸âƒ£ Creating Linked Services
In Azure Data Factory, I created two linked services:

Azure Blob Storage Linked Service â€“ Connects to my storage account.
Azure SQL Database Linked Service â€“ Connects to my SQL database.
2ï¸âƒ£ Creating Datasets
Datasets help ADF read/write data from different sources. I created:

ğŸ”¹ Datasets for Azure Blob Storage
Employees_Initial_DS â†’ Points to Employees_Initial.csv in Blob Storage.
Employees_Updated_DS â†’ Points to Employees_Updated.csv in Blob Storage.
ğŸ”¹ Dataset for Azure SQL Database
Employees_SQL_DS â†’ Points to the Employees table in SQL Database.
ğŸ“Œ 3. Loading Initial Employee Data into SQL
Pipeline: LoadInitialEmployees
Source â†’ Employees_Initial_DS (from Blob Storage).
Sink â†’ Employees_SQL_DS (to SQL Database).
Mapping â†’ Mapped columns:
EmployeeID â†’ EmployeeID
Name â†’ Name
Salary â†’ Salary
Location â†’ Location
Executed Pipeline â†’ Successfully inserted records into the SQL database.
ğŸ“Œ 4. Implementing SCD Type 1 using ADF Data Flow
ğŸ”¹ Step-by-Step Breakdown of the Data Flow
I created a Data Flow named SCDType1DataFlow and added the following transformations:

1ï¸âƒ£ Source: Employees_Updated
Reads the updated employee dataset (Employees_Updated.csv) from Blob Storage.
2ï¸âƒ£ Source: Employees_DB
Reads the existing employee records from the SQL Database.
3ï¸âƒ£ Join Transformation (Left Outer Join)
Primary Stream: Employees_Updated (Latest data).
Lookup Stream: Employees_DB (Existing data).
Join Condition: Employees_Updated.EmployeeID == Employees_DB.EmployeeID.
This allows us to compare new data with existing records.
4ï¸âƒ£ Derived Column Transformation
Created two new columns:
IsUpdated â†’ if(Employees_DB.Salary != Employees_Updated.Salary, 1, 0) (Checks if salary has changed).
IsNew â†’ if(isNull(Employees_DB.EmployeeID), 1, 0) (Checks if the employee is new).
5ï¸âƒ£ Filter Transformation
Filter 1 (Updated Employees) â†’ IsUpdated == 1 (Extracts employees with salary updates).
Filter 2 (New Employees) â†’ IsNew == 1 (Extracts new employees).
6ï¸âƒ£ Alter Row Transformation
Condition: Update if IsUpdated == 1.
Condition: Insert if IsNew == 1.
7ï¸âƒ£ Sink Transformation
Sink 1 â†’ Updates employees in SQL Database.
Sink 2 â†’ Inserts new employees into SQL Database.
ğŸ“Œ 5. Running & Validating the Pipeline
Ran the Pipeline â†’ Successfully processed the updated employee data.

